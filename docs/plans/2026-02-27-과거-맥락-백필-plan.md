# 과거 맥락 데이터 백필 구현 계획

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Linear 이슈와 GitHub 활동(커밋, PR)의 과거 데이터를 맥락 수집 시스템(PostgreSQL `context_events` 테이블)에 일회성으로 백필한다.

**Architecture:** n8n 워크플로우 2개(Linear, GitHub)를 수동 트리거로 각 1회 실행하여 과거 데이터를 수집한다. 각 워크플로우는 Code 노드에서 API 호출 + 페이지네이션 + 파싱을 처리하고, PostgreSQL UPSERT로 저장한다. 실행 후 워크플로우를 삭제한다.

**Tech Stack:** n8n 워크플로우 (MCP), Linear GraphQL API, GitHub REST API, PostgreSQL

**참조:**
- 기존 수집 워크플로우: Linear(`zDBSF0HhTzG3Fl77`), GitHub(`4khlESRhzRArHsEU`)
- 조회 API: `NJu5CxFmsq3VXoii`
- PostgreSQL credential: `gOmQnteqITlWTU6y` ("Postgres account")
- n8n gotchas: `.claude/skills/n8n-gotchas/SKILL.md`

---

## Task 0: 사전 준비 — API 키/토큰 확보

**목표:** Linear API 키, GitHub PAT, 백필 대상 리포 목록을 확보한다.

**Step 1: 사용자에게 필요 정보 요청**

AskUserQuestion으로 다음 3가지를 요청한다:

1. **Linear Personal API Key**
   - 생성 위치: Linear Settings > Account > Security & Access > "Personal API keys"
   - 형식: `lin_api_XXXXX`

2. **GitHub Personal Access Token (Fine-grained)**
   - 생성 위치: GitHub Settings > Developer settings > Personal access tokens > Fine-grained tokens
   - 필요 권한: Contents (Read), Pull requests (Read)
   - 대상: 백필할 모든 리포지토리
   - 형식: `github_pat_XXXXX`

3. **백필 대상 GitHub 리포 목록**
   - 알려진 리포: `Unuuuuu/happiness`
   - 추가 리포가 있으면 `owner/repo` 형태로 나열

**중요:** API 키/토큰은 일회성 백필 후 워크플로우와 함께 삭제된다. 영구 저장되지 않는다.

---

## Task 1: Linear 과거 이슈 백필 워크플로우

**목표:** Linear의 모든 이슈를 `context_events` 테이블에 백필한다.

**Step 1: 워크플로우 생성**

`n8n_create_workflow`로 워크플로우를 생성한다:
- 이름: `Backfill: Linear Issues`
- 구조: Manual Trigger → Code (`fetch_linear_issues`) → PostgreSQL (`upsert_events`)

**Code 노드 (`fetch_linear_issues`) 코드:**

```javascript
// Linear 과거 이슈 백필
// {{LINEAR_API_KEY}}를 Task 0에서 받은 실제 키로 교체
const API_KEY = '{{LINEAR_API_KEY}}';

const query = `query($cursor: String) {
  issues(first: 50, after: $cursor, orderBy: updatedAt) {
    nodes {
      id title identifier description priority priorityLabel url
      createdAt updatedAt
      state { name }
      assignee { name }
      creator { name }
      project { name }
      team { name }
      labels { nodes { name } }
    }
    pageInfo { hasNextPage endCursor }
  }
}`;

const items = [];
let cursor = null;
let hasNext = true;
let pageCount = 0;

while (hasNext) {
  const res = await this.helpers.httpRequest({
    method: 'POST',
    url: 'https://api.linear.app/graphql',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': API_KEY,
    },
    body: { query, variables: { cursor } },
  });

  const { nodes, pageInfo } = res.data.issues;
  pageCount++;

  for (const issue of nodes) {
    items.push({
      json: {
        source: 'linear',
        event_type: 'issue_updated',
        entity_id: issue.id,
        title: issue.title,
        status: issue.state?.name || 'Unknown',
        metadata: JSON.stringify({
          identifier: issue.identifier,
          priority: issue.priority,
          priorityLabel: issue.priorityLabel,
          labels: (issue.labels?.nodes || []).map(l => l.name),
          description: (issue.description || '').substring(0, 500),
          url: issue.url,
          team: issue.team?.name,
        }),
        actor: issue.assignee?.name || issue.creator?.name || 'Unknown',
        project: issue.project?.name || issue.team?.name || 'Unknown',
        updated_at: issue.updatedAt,
      }
    });
  }

  hasNext = pageInfo.hasNextPage;
  cursor = pageInfo.endCursor;
}

return items;
```

**PostgreSQL 노드 (`upsert_events`) 설정:**

- operation: `executeQuery`
- credential: `gOmQnteqITlWTU6y` ("Postgres account")
- query:

```sql
INSERT INTO context_events (source, event_type, entity_id, title, status, metadata, actor, project, updated_at)
VALUES ($1, $2, $3, $4, $5, $6::jsonb, $7, $8, $9)
ON CONFLICT (source, entity_id) DO UPDATE SET
  event_type = EXCLUDED.event_type,
  title = EXCLUDED.title,
  status = EXCLUDED.status,
  metadata = EXCLUDED.metadata,
  actor = EXCLUDED.actor,
  project = EXCLUDED.project,
  updated_at = EXCLUDED.updated_at
```

- queryReplacement: `={{ $json.source }},={{ $json.event_type }},={{ $json.entity_id }},={{ $json.title }},={{ $json.status }},={{ $json.metadata }},={{ $json.actor }},={{ $json.project }},={{ $json.updated_at }}`

**Step 2: 워크플로우 검증**

`n8n_validate_workflow`로 구조를 검증한다.

**Step 3: 워크플로우 실행**

`n8n_test_workflow`로 수동 실행한다.

기대 결과:
- Code 노드가 Linear API를 호출하여 모든 이슈를 가져옴
- 각 이슈가 PostgreSQL에 UPSERT됨
- 에러 없이 완료

**Step 4: 데이터 확인**

WebFetch로 조회 API를 호출하여 Linear 데이터를 확인:

```
GET https://unu.unuuuuu.com/webhook/get-my-context?days=365&source=linear
```

기대 결과: 백필된 Linear 이슈들이 상태별로 분류되어 표시됨.

---

## Task 2: GitHub 과거 활동 백필 워크플로우

**목표:** GitHub의 커밋과 PR을 `context_events` 테이블에 백필한다.

**참고:** GitHub Events API는 30일 보존 제한 + 최대 300건이므로 사용하지 않는다. 대신 commits API와 pulls API를 사용한다.

**Step 1: 워크플로우 생성**

`n8n_create_workflow`로 워크플로우를 생성한다:
- 이름: `Backfill: GitHub Activity`
- 구조: Manual Trigger → Code (`fetch_github_activity`) → PostgreSQL (`upsert_events`)

**Code 노드 (`fetch_github_activity`) 코드:**

```javascript
// GitHub 과거 활동 백필
// {{GITHUB_PAT}}와 REPOS를 Task 0에서 받은 값으로 교체
const TOKEN = '{{GITHUB_PAT}}';
const REPOS = [{{REPO_LIST}}]; // e.g., ['Unuuuuu/happiness']
const SINCE_DAYS = 90;

const headers = {
  'Authorization': `Bearer ${TOKEN}`,
  'Accept': 'application/vnd.github.v3+json',
  'User-Agent': 'n8n-backfill',
};

const items = [];
const since = new Date();
since.setDate(since.getDate() - SINCE_DAYS);

for (const repo of REPOS) {
  const repoName = repo.split('/')[1];

  // --- PR 수집 ---
  let page = 1;
  while (true) {
    const prs = await this.helpers.httpRequest({
      method: 'GET',
      url: `https://api.github.com/repos/${repo}/pulls`,
      qs: { state: 'all', sort: 'updated', direction: 'desc', per_page: 100, page },
      headers,
    });
    if (!prs.length) break;

    for (const pr of prs) {
      if (new Date(pr.updated_at) < since) continue;
      items.push({
        json: {
          source: 'github',
          event_type: 'pull_request',
          entity_id: `pr-${repoName}-${pr.number}`,
          title: pr.title,
          status: pr.merged_at ? 'merged' : pr.state,
          metadata: JSON.stringify({
            number: pr.number,
            branch: pr.head?.ref,
            base: pr.base?.ref,
            url: pr.html_url,
            draft: pr.draft || false,
          }),
          actor: pr.user?.login || 'Unknown',
          project: repo,
          updated_at: pr.updated_at,
        }
      });
    }

    // 가장 오래된 PR이 since 이전이면 중단
    if (new Date(prs[prs.length - 1].updated_at) < since) break;
    page++;
  }

  // --- 커밋 수집 ---
  page = 1;
  while (true) {
    const commits = await this.helpers.httpRequest({
      method: 'GET',
      url: `https://api.github.com/repos/${repo}/commits`,
      qs: { since: since.toISOString(), per_page: 100, page },
      headers,
    });
    if (!commits.length) break;

    for (const c of commits) {
      items.push({
        json: {
          source: 'github',
          event_type: 'push',
          entity_id: `commit-${repoName}-${c.sha.substring(0, 7)}`,
          title: (c.commit?.message || '').split('\n')[0],
          status: 'completed',
          metadata: JSON.stringify({
            sha: c.sha,
            url: c.html_url,
            branch: 'main',
          }),
          actor: c.author?.login || c.commit?.author?.name || 'Unknown',
          project: repo,
          updated_at: c.commit?.author?.date,
        }
      });
    }
    page++;
  }
}

return items;
```

**PostgreSQL 노드 (`upsert_events`) 설정:**

Task 1과 동일한 PostgreSQL 설정을 사용한다 (query, queryReplacement, credential 모두 동일).

**Step 2: 워크플로우 검증**

`n8n_validate_workflow`로 구조를 검증한다.

**Step 3: 워크플로우 실행**

`n8n_test_workflow`로 수동 실행한다.

기대 결과:
- Code 노드가 GitHub API를 호출하여 PR과 커밋을 가져옴
- 각 항목이 PostgreSQL에 UPSERT됨
- 에러 없이 완료

**Step 4: 데이터 확인**

WebFetch로 조회 API를 호출하여 GitHub 데이터를 확인:

```
GET https://unu.unuuuuu.com/webhook/get-my-context?days=365&source=github
```

기대 결과: 백필된 GitHub 커밋과 PR이 타입별로 분류되어 표시됨.

---

## Task 3: 통합 검증 및 정리

**목표:** 전체 데이터를 확인하고 백필 워크플로우를 삭제한다.

**Step 1: 통합 데이터 확인**

WebFetch로 전체 데이터를 확인:

```
GET https://unu.unuuuuu.com/webhook/get-my-context?days=365
```

기대 결과:
- Linear 이슈가 상태별로 표시됨 (In Progress, Todo, Done 등)
- GitHub PR과 커밋이 타입별로 표시됨
- 14일 이전 데이터가 포함되어 있음

**Step 2: /work-scan 재실행**

`/work-scan` 스킬을 호출하여 백필된 데이터로 의미 있는 패턴이 나오는지 확인한다.

기대 결과:
- 현황 요약에 더 많은 데이터가 표시됨
- 패턴 분석에서 장기 체류, 우선순위 불일치 등 실질적 패턴이 탐지됨

**Step 3: 백필 워크플로우 삭제**

`n8n_delete_workflow`로 두 백필 워크플로우를 삭제한다:
- `Backfill: Linear Issues`
- `Backfill: GitHub Activity`

일회성 용도이므로 삭제하여 API 키/토큰이 n8n에 남지 않게 한다.

**Step 4: 커밋**

```bash
git add docs/plans/2026-02-27-과거-맥락-백필-plan.md
git commit -m "docs: 과거 맥락 데이터 백필 구현 계획 작성"
```
